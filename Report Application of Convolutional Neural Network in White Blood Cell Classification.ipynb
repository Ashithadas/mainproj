{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project Report\n",
    "Xi Palazzolo\n",
    "September 28, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition\n",
    "***White Blood Cell***\n",
    "\n",
    "All definitions are adopted from the National Cancer Institute. White blood cell is a type of blood cell that is made in the bone marrow and found in the blood and lymph tissue. White blood cells are part of the body’s immune system. They help the body fight infection and other diseases. Types of white blood cells are granulocytes (neutrophils, eosinophils, and basophils), monocytes, and lymphocytes (T cells and B cells). Checking the number of white blood cells in the blood is usually part of a complete blood cell (CBC) test. It may be used to look for conditions such as infection, inflammation, allergies, and leukemia. Also called leukocyte and WBC.\n",
    "- **Granulocytes**\n",
    "    - Neutrophil: A type of immune cell that is one of the first cell types to travel to the site of an infection. Neutrophils help fight infection by ingesting microorganisms and releasing enzymes that kill the microorganisms.            \n",
    "                \n",
    "    - Eosinophil: A type of immune cell that has granules (small particles) with enzymes that are released during infections, allergic reactions, and asthma.   \n",
    "                  \n",
    "    - Basophil (will not be included for classification in this project): A type of immune cell that has granules (small particles) with enzymes that are released during allergic reactions and asthma.   \n",
    "    \n",
    "                    \n",
    "- **Monocytes**\n",
    "\n",
    "A type of immune cell that is made in the bone marrow and travels through the blood to tissues in the body where it becomes a macrophage. Macrophages surround and kill microorganisms, ingest foreign material, remove dead cells, and boost immune responses. A monocyte is a type of white blood cell and a type of phagocyte.    \n",
    "\n",
    "- **Lymphocytes**\n",
    "\n",
    "A type of immune cell that is made in the bone marrow and is found in the blood and in lymph tissue. The two main types of lymphocytes are B lymphocytes and T lymphocytes. B lymphocytes make antibodies, and T lymphocytes help kill tumor cells and help control immune responses. \n",
    "\n",
    "<img src='whitecells.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "White blood cell (WBC) differenciation plays an important role in clinical procedures that provides invaluable information to medical professionals for diagnose diseases in patients. Many blood diseases such as leukemia, acquired immunodeficiency syndrome (AIDS), or cancers can be diagonsed by processing the white blood cell differencial counts, ie., the counts of different cell classes (Theera-Umpon and Dhompongsa, 2007). Cell classification has widespread interest especially for clinics and laboratories. For example, patient’s blood cells\n",
    "counting is use to extract information about other cells that are not normally present in peripheral blood but may be released in certain disease processes by the hematologists. One of the great challenge to engineer especially biomedical engineer is to transform this human practical task into computer based which the system is comparable to human performance or better. Thus, the system must be stable and able to handle the uncertainty. Up to now, automatic cell classification systems can not meet the complexity of real clinical demands.\n",
    "\n",
    "Most of the proposed automatic techniques follow the traditional manual process of detecting a cell, extracting its features, classifying the cell, and then updating the counts. Ritter et al. presented a fully automatic method for segmentation and border identification of all objects that do not overlap the boundary in an image taken from a peripheral blood smear slide. In their work, pale tips of protuberances are lost. Ongun et al. did segmentation by morphological preprocessing followed by the snake-balloon algorithm. Jiang et al. proposed a WBC segmentation scheme on color space images using feature space clustering techniques, scale-space filtering for nucleus extraction, and watershed clustering for cytoplasm extraction. Leyza et al. used morphological operators and examined the scale-space properties of toggle operator to improve segmentation accuracy. Scotti presented the automatic morphological method that is based on the morphological analysis of WBCs. Their proposed system extracts the morphological indexes (lymphocytes). Kumar et al. used teager energy operator for segmentation, nucleus based on the edges, which are detected effectively by teager energy operator but it required at least a weak edge to exist between red blood cell (RBC) and the background. \n",
    "\n",
    "At the personal level, the motivation for this project came from my background in veterinary medicine. I was practicing as a veterinarian before changing my career path to data science. My former experience in clinical practices often involved blood cell classification on a daily basis, which was quite time-consuming and tedious. Therefore, I feel that if there is an algorithm built into a microscope that can greatly enhance diagosis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Traditionally, blood cell classification is a rather complicated and frequently burdensome process. Patient’s blood cells counting were performed manually by medical technologists by viewing slide prepared with blood sample of the patient under microscope. A manual count will also give information about other cells that are not normally present in peripheral blood but might be released in certain disease. Unfortunately, the accuracy of cell classification and counting is strongly affected by individual operator’s capabilities. In particular, the identification and differential count of blood’s cell is a timeconsuming and repetitive task that can be influenced by operator’s accuracy and tiredness (Adollah et al., 2008). Thus, an automated counting system or algorithm is highly desired to help support such diagnostic procedures, which if effective, could potentially reduce cost and provide more timely information for patients' diagnosis and prognosis. This problem can be measured by the accuracy of the classification of cell classes: Eosinophil, Lymphocyte, Monocyte, and Neutrophil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Accuracy classfication score will be used to evaluate the performance of the model. Specifically, the accuracy_score function from sklearn.metrics will be used. \n",
    "\n",
    "The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.In multilabel classification, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.If  is the predicted value of the -th sample and  is the corresponding true value, then the fraction of correct predictions over  is defined as\n",
    "\n",
    "<img src='accuracy_score_formula.png'>\n",
    "\n",
    "where  is the indicator function. In this project, the main goal is to correctly classify input images to the appropriate categories. That means, the performance of the classifier would be the number of images that it correctly classifies. Therefore, accuracy score would be the optimal choice of metric to be used for evaluating the performance of the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this project is a publicly available dataset (https://www.kaggle.c-om/paultimothymooney/blood-cells/data), originated from the BCCD dataset (retrieved from https://github.com/Shenggan/BCCD_Dataset).The BCCD dataset is a small-scale dataset for blood cell detection. The latter dataset contains 12,500 augmented images of blood cells (JPEG) with accompanying cell type labels (CSV). This dataset is accompanied by an additional dataset containing the original 410 images (pre-augmentation) as well as two additional subtype labels (WBC vs WBC) and also bounding boxes for each cell in each of these 410 images (JPEG + XML metadata). Each original image of the BCCD dataset is of size 640 x 480. After augmentation, each image is of size 120 x 160 so I can train the model faster.\n",
    "\n",
    "The dataset obtained from Kaggle comprises of training and test sets, including 9,957 training and 2,487 testing images. In the training set, there are \n",
    "- 2,497 eosinophil cell images, \n",
    "- 2,483 lymphocyte cell images, \n",
    "- 2,478 monocyte cell images, and \n",
    "- 2,499 neutrophil cell images.\n",
    "\n",
    "In the testing set, there are \n",
    "- 623 eosinophil cell images, \n",
    "- 620 lymphocyte cell images, \n",
    "- 620 monocyte cell images, and \n",
    "- 624 neutrophil cell images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29215a0f278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "set_1 = {'Cell_Type': ['Eosinophil', 'Lymphocyte', 'Monocyte', 'Neutrophil'], \n",
    "         'Sample_Size': [2497, 2483, 2478, 2499]}\n",
    "\n",
    "data = pd.DataFrame(set_1)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(data.Cell_Type, data.Sample_Size, align = 'center', alpha = 0.5)\n",
    "plt.xlabel('Cell Type')\n",
    "plt.ylabel('Sample Size')\n",
    "plt.title('Distribution of Sample Size by Cell Types')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above bar chart shows the distribution of the training dataset by cell types, which indicates a quite balance among cell types. A balanced training dataset in a classification problem can avoid a permanent definition bias. However, whether or not to balance a training dataset also depends on the main goal of the classification problem. For example, if my goal is to identify lymphocyte, then I should add in more lymphocyte images in the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of the Dataset**\n",
    "\n",
    "<img src=\"Neutrophil.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm and Techniques \n",
    "\n",
    "One algorithmatic solution to this problem is through the use of convolutional neural network (CNN). CNNs are models proposed by the development of biotechnology. Neurons are like local filtering of the entire input space, and they are well-organized together to achieve an understanding of the image in the entire field of view. CNN may extract the local and deep features of the input image. The performance of this model can be measured by its classification accuracy. \n",
    "\n",
    "***Algorithms Used***\n",
    "- Transfer Learning is used in this project to solve the proposed problem. In Transfer Learning, the knowledge of an already trained Machine Learning model is applied to a different but related problem. The general idea is to use knowledge, that a model has learned from a task where a lot of labeled training data is available, in a new task where we don’t have a lot of data. Instead of starting the learning process from scratch, you start from patterns that have been learned from solving a related task (Niklas Donges, 2018). For example, in computer vision, Neural Networks usually try to detect edges in their earlier layers, shapes in their middle layer and some task-specific features in the later layers. With transfer learning, you use the early and middle layers and only re-train the latter layers. It helps us to leverage the labeled data of the task it was initially trained on. So in this project, by using pre-trained models, the classifiers would be able to identify the object in our data--cells. What I needed to do was to train the latter layers to classifier its type. \n",
    "\n",
    "-  VGG-16: VGG-16 is trained using ImageNet data and is characterized by its simplicity, using only 3×3 convolutional layers stacked on top of each other in increasing depth. Reducing volume size is handled by max pooling. Two fully-connected layers, each with 4,096 nodes are then followed by a softmax classifier. This model has 16 weight layers.\n",
    "\n",
    "- Inception V3: The goal of the inception module is to act as a “multi-level feature extractor” by computing 1×1, 3×3, and 5×5 convolutions within the same module of the network — the output of these filters are then stacked along the channel dimension and before being fed into the next layer in the network. The original incarnation of this architecture was called GoogLeNet, but subsequent manifestations have simply been called Inception vN where N refers to the version number put out by Google.\n",
    "\n",
    "- Xception: Xception is an extension of the Inception architecture which replaces the standard Inception modules with depthwise separable convolutions. The original publication, Xception: Deep Learning with Depthwise Separable Convolutions can be found here.\n",
    "\n",
    "***Techniques Used***\n",
    "- Data Splitting: Usually, if a dataset is large enough, I split it into three parts--training, validation, and test. In this project, I used sklearn.model_selection.train_test_split modul to split my training data into training and validation. One important parameter here is the ratio, test_size. The common ratio is 0.3 or 0.33. \n",
    "\n",
    "- Data Pre-Processing: Every pre-trained CNN model has its default input size. Also, image  Therefore, I pre-processed all input images as the first step. Images were loaded and converted into a 4D array. Then each image was rescaled by dividing every pixel by 255.  \n",
    "\n",
    "- Image Augmentation: This is an important step in image classification problems. However, this technique was not performed in this project since the dataset has already been augmented. Usually, I would create a image generator to rotate and flip an image by a set angle using ImageDataGenerator. Important parameters include rotation_range--the degree of rotation and horizontal_shift.  \n",
    "\n",
    "- Feature Extraction: In this section, I used selected pre-trained models to detect and isolate the desired shapes (features) of the inpute images--cell images used for later training. \n",
    "\n",
    "- Build the Model Architecture: In all of my models, there are three layers--pre-trained CNN layer, convolutional layer, pool layer, and a fully-connected layer. In the convolutional layer, ReLu activation method was selected. An activation function is just like a weight function, calculating a “weighted sum” of its input, adding a bias and then deciding whether a neuron should be “fired” or not. ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations and therefore is selected in this project. Another parameter to pay attention is the input_shape. One needs to ensure the input_shape equal to the input_shape of the pre-process dataset. In the last layer-the fully connected layer, I used 'softmax' as the activation method, which would give out probability of each output. \n",
    "\n",
    "- Model Compiling: This is a technique to commision the model. An important parameter is the loss function and optimizer. Since this project handles a classification problem, 'categorical_crossentropy' loss function was used in the model. Because RMSprop divides the learning rate by an exponentially decaying average of squared gradients and was selected as the method for optimizing the model. As well, I added Dropout layer to avoid overfitting. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "In 2018, Liang and his colleagues published a model classifying white blood cell classes using the same datasets. In their research, they propose an architecture that combines CNN and recurrent neural network (RNN). According to Liang et al., RNNs are a neural network used to process sequence data. In the traditional neural network model, from the input layer to the hidden layer to the output layer, the layers are connected with each other, and the nodes between each layer are connected. Such network model cannot handle this kind of sequential data. Socher et al. (2012a) presented a convolutional-recursive deep model for 3D object classification that combined the convolutional and recurrent neural networks (CNN and RNN) together. The CNN layer learns low-level translation invariant features and then uses it as input to multiple fixed-tree RNNs to form higher order features. RNNs can be seen as combining convolution and pooling into one efficient, hierarchical operation. Kim et al.described a model that employed a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long shortterm memory (LSTM) recurrent neural network language model (RNN-LM). These two models both obtain better results compared to a-priori methods.\n",
    "\n",
    "Liang et al.'s model consists of the following parts: Pre-trained convolutional neural network layer, RNN layer, Merge layer, and fully connected layer with Softmax output. This model integrates the local features extracted from the CNN and the features obtained from the RNN to perform the blood cell classification. They used the CNN model pre-trained on the ImageNet dataset and retains its weight parameters. Liang and colleagues perceived the CNN and RNN as two separate branches. First, they freezed the pre-trained CNN model, used the pre-processed training data as the input of the RNN model, extract and save the obtained features, and merge the features from the RNN and CNN. The RNN weight parameters were constantly updated during training. Finally, they thawed all network layers and use the training data as input to the CNN model and the RNN model. In the CNN model, they applied different size and weight matrix windows in order to generate multiple feature maps. The features extracted from the RNN model and the features extracted from the CNN model are combined according to the corresponding element multiplication methods. And then, the classification results were displayed using Softmax. In addition, they also used a fine-tuning strategy to retrain the CNN-RNN framework, and finally obtained the classification results. Accuracy score was used to evaluate the performance of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The dataset used in this project contains pre-augmented images of white blood cells of 320 X 240 X 3 pixels. When using TensorFlow as backend, Keras CNNs require a 4D array (which is also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "                                        (nb_samples,rows,columns,channels),\n",
    "\n",
    "where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.\n",
    "\n",
    "The path_to_tensor function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN. The function first loads the image and resizes it to a square image that is  224×224  pixels. Next, the image is converted to an array, which is then resized to a 4D tensor. In this case, since we are working with color images, each image has three channels. Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "                                                    (1,224,224,3).\n",
    "                                                    \n",
    "As well, each image was rescale by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation \n",
    "\n",
    "In my model, there are four layers: \n",
    "\n",
    "1) Pre-trained CNN Layer\n",
    "\n",
    "I will use the weight parameters obtained by pre-training on the ImageNet dataset as the initialization weights of my CNN model. CNNs include convolutional layer and pooling layer.\n",
    "\n",
    "2) Convolutional Layer\n",
    "\n",
    "As the most important part of the convolutional neural network, the main way to calculate this layer is to use convolution windows with different sizes in order to perform convolution operations with the feature maps of the previous layer. Convolution windows of different sizes slide in sequence onto the feature map of the previous layer. The window size is usually 3×3 or 5×5, and the number of weight parameters of the convolutional layer also changes accordingly. The values of the neurons on each feature map in the convolutional layer are convoluted through corresponding windows, and then the final result is obtained based on the excitation function used in the layer. In the convolutional layer, ReLu activation method was selected. An activation function is just like a weight function, calculating a “weighted sum” of its input, adding a bias and then deciding whether a neuron should be “fired” or not. ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations and therefore is selected in this project. Another parameter to pay attention is the input_shape. One needs to ensure the input_shape equal to the input_shape of the pre-process dataset.\n",
    "\n",
    "3) Pool Layer\n",
    "\n",
    "The calculation process of this layer is similar to the operation of the convolutional layer. The difference is that the sliding window of the lower sampling layer is usually 2 × 2, and the sliding step is 2. Therefore, this process will usually halved the feature map of the size of the previous layer, which to a large extent can greatly reduce the convolution weights of neural network parameters, the number of there are very good for the overall speed of the network training process to promote. At the same time, it also enables the network to become more adaptive to the scale of the image changes. In this project, I select the pre-train model -- Xception, and I will use the ReLU (Linear Rectification Function) as my activation function. Xception is another improved model of Google’s Inception v3. It mainly implements depthwise separable convolution to replace the original convolution operation in Inception v3. Xception, as an improved model for Inception v3, mainly introduces depthwise separable convolution on the basis of Inception v3, which improves the model’s effect without increasing network complexity. In this layer, global average pooling (GAP) method was selected to minimize overfitting by reducing the total number of parameters in the model. GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. In addition, GAP layers perform a more extreme type of dimensionality reduction, where a tensor with dimensions h×w×d is reduced in size to have dimensions 1×1×d. GAP layers reduce each h×w feature map to a single number by simply taking the average of all hw values.\n",
    "\n",
    "4) Fully Connected Layer with Softmax Output\n",
    "\n",
    "After features are generated by CNN layer, I will pass them to the fully connected Softmax layer, the output of which is the probability distribution of all classes. In addition, I will use the cross-entropy as a loss function to measure the difference between the actual output and the target output. In addition, a dropout layer is also added to reduce overfitting. \n",
    "\n",
    "During the coding process, at first I didn't notice the input_shape not being consistent which causing the model not being able to compile. After making sure all input_shape being consistent, I was able to compile the models without any other complications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "In the training process, I refined the number of epochs trained in each model based on the training accuracy score. At first, I defaulted all training epochs to 50, where not all models achieved an ideal accuracy score. By printing out the accuracy score against each epoch, I was able to observe the changes in the score. Therefore, I ajusted the number of epochs for each model in order to achieve the highest accuracy score possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Results\n",
    "\n",
    "***VGG-16 Model Architecture***\n",
    "<img src='vgg16_model.png'>\n",
    "\n",
    "***VGG_16 Results***\n",
    "\n",
    "<img src='vgg_16_result.png'>\n",
    "<img src='vgg_loss.png'>\n",
    "\n",
    "From the graphs above, the VGG-16 accuracy score pattern matches the loss pattern where its test set achieved higher accuracy score than training set and in the reverse pattern in the loss. \n",
    "\n",
    "***Xception Model Architecture***\n",
    "<img src='xception_model.png'>\n",
    "\n",
    "***Xception Results***\n",
    "<img src='xception_result.png'>\n",
    "<img src='xception_loss.png'>\n",
    "\n",
    "From the charts above, I noticed that although the Xception model had higher accuracy score in the training set, its testing accuracy score however was lower while having higher loss. \n",
    "\n",
    "***Inception V3 Model Architecture***\n",
    "<img src='inception_model.png'>\n",
    "\n",
    "***Inception V3 Results***\n",
    "<img src='inception_result.png'>\n",
    "<img src='inception_loss.png'>\n",
    "\n",
    "From the above charts above, Inception V3 had the worst performance in both accuracy score and loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation \n",
    "\n",
    "The accuracy and model loss were used to compare the performance of models built with three pre-trained CNN models--VGG16, Xception, and Inception V3. It turned out model built with VGG-16 returned the highest testing accuracy rate and the lowest model loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification \n",
    "\n",
    "The result in this application was different than that of the benchmark model, where the model built with Xception achieved the highest accuracy rate. One main reason attributing to such variance in the result is the architecture of models. In the benchmark model, the authors froze the CNN pre-trained model while adding a layer of recurrent neural network that further enhanced the performance of the model. In this application, only CNN was used in the architecture. The highest testing accuracy rate achieved in this application was by VGG-16 of 60.43%. Because this was for clinical diagnostic use, a higher accuracy rate was expected. Further modications are requred to obtain a more confident classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='accuracy_score_performance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a comparison of training and testing accuracy scores among three different models. Of these, Inception V3 had the lowest training and testing accuracy scores while seemingly volatile. Interestingly, different than the result of the benchmark model, in this project, VGG-16 achieved the highest test accuracy score, although the training accuracy score was not high. Although Xception achieved the highest training accuracy score, its test accuracy score was not as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='model_loss_performance.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, categorical cross entropy is used as the loss function. Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. A perfect model would have a log loss of 0. Above is a comparison of model losses among three different models. Similar to their performance in accuracy score, VGG-16 is the best performer in test data while Xception performed best in training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection \n",
    "\n",
    "For this white blood cell classification problem, before diving into the actual training and testing process, I first imported and pre-processed the dataset like many other image recognition problem processes do. The normal process includes image dimension processing, image rescaling, and image augmentation. But since the dataset I used in this project already contains augemented images, I did not conduct image augmentation in the pre-processing. Secondly, I split the dataset into training, validation, and testing sets. Then, I extracted bottleneck features using pre-trained CNN models and fitted them to the dataset. Next, I developed the architecture of my CNN models using three different pre-trained CNN models. In the process of training my models, I observed and tuned the number of training epochs based on the resulting training accuracy and model loss values. After that, the models were put to test using the testing dataset. In order to compare and evaluate the performance of these three models, I visualized their accuracy scores and model loss values throughout all epochs. \n",
    "\n",
    "One interesting and difficult part of the project was adjusting the number of training epochs. I noticed that higher number of training epochs did not always correlate with higher training accuracy. \n",
    "\n",
    "The final solution of my choice of pre-trained model (Xception) did not really achieve my expectation because it under-performed compared to VGG-16. However, I think that with further refinements, this classifier can be used to broader cell classifications such as platelets, sickle cells, and even tumor cells. With this software-first approach to morphology, I think we can apply Machine Learning to healthcare in a meaningful, valuable way. Most importantly I hope that we can enable:\n",
    "\n",
    "- Faster iteration cycles and improvements (as with all software).\n",
    "- Increased accessibility to high quality, quantitative assessments.\n",
    "- Lower costs and better patient outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement \n",
    "\n",
    "I think the algorithm and the architecture of the model can be further refined as suggested in the benchmark model. As indicated by the authors of the benchmark model, the incorporation of CNN pre-trained model and RNN can greatly enhance the accuracy of the classifier. As well, in the pre-trained CNN model, they used the pre-processed training data as the input of the RNN model, extract and save the obtained features, and merge the features from the RNN and CNN. The RNN weight parameters were constantly updated during training. Finally, they thawed all network layers and use the training data as input to the CNN model and the RNN model. In the CNN model, they applied different size and weight matrix windows in order to generate multiple feature maps. The features extracted from the RNN model and the features extracted from the CNN model are combined according to the corresponding element multiplication methods. These are the things that can be improved in my model to further enhance the performance of the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "- Adollah R., Mashor M.Y., Mohd Nasir N.F., Rosline H., Mahsin H., Adilah H. 2008. Blood Cell Image Segmentation: A Review. In: Abu Osman N.A., Ibrahim F., Wan Abas W.A.B., Abdul Rahman H.S., Ting HN. (eds) 4th Kuala Lumpur International Conference on Biomedical Engineering 2008. IFMBE Proceedings, vol 21. Springer, Berlin, Heidelberg.\n",
    "\n",
    "- Dorini LB, Minetto R, Leite NJ: White blood cell segmentation using morphological operators and scale-space analysis. 2007. SIBGRAPI '07: Proceedings of the XX Brazilian Symposium on Computer Graphics and Image Processing. \n",
    "\n",
    "- Jiang Kan, Liao Qing-Min, Dai Sheng-Yang: A novel white blood cell segmentation scheme using scale-space filtering and watershed clustering. Machine Learning and Cybernetics, 2003 International Conference on. 2003, 5: 2820-2825.\n",
    "\n",
    "- Kim Y, Y. Jernite, D. Sontag, and A. M. Rush. 2015. Character-aware neural language models. Computer Science.\n",
    "\n",
    "- Kumar BR, Joseph DK, Sreenivasc TV. 2002. Teager energy based blood cell segmentation. Digital Signal Processing. DSP 2002 14th International Conference on. 2: 619-622.\n",
    "\n",
    "- GAOBO LIANG , HUICHAO HONG, WEIFANG XIE, AND LIXIN ZHENG. 2018. Combining Convolutional Neural Network With Recursive Neural Network for Blood Cell Image Classification. Institute of Electrical and Electronics Engineers. 6: 36188-36197. \n",
    "\n",
    "- National Cancer Institute. Last accessed on August 10, 2018: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/white-blood-cell\n",
    "\n",
    "- Nipon Theera-Umpon and Sompong Dhompongsa. 2007. Morphological Granulometric Features of Nucleus in Automatic Bone Marrow White Blood Cell Classification. IEEE TRANSACTIONS ON INFORMATION TECHNOLOGY IN BIOMEDICINE, VOL. 11, NO. 3.\n",
    "\n",
    "- Niklas Donges, 2018. Transfer Learning. Retrieved from: https://towardsdatascience.com/transfer-learning-946518f95666\n",
    "\n",
    "- Ongun G, Halici U, Leblebicioglu K, Atalay V, Beksac M, Beksac S. 2001. Feature extraction and classification of blood cells for an automated differential blood count system. Neural Networks. Proceedings. IJCNN '01. International Joint Conference on. 4: 2461-2466.\n",
    "\n",
    "- Ritter N, Cooper J. 2007. Segmentation and border identification of cells in images of peripheral blood smear slides. Proceedings of the Thirtieth Australasian Conference on Computer Science. 62: 161-169.\n",
    "\n",
    "-  Socher R, B. Huval, B. Bath, C. D. Manning, and A. Y. Ng. 2012. Convolutional-recursive deep learning for 3D object classification. Proceedings of Neural Information Processing Systems. 656–664.\n",
    "\n",
    "- Scotti F. 2005. Automatic morphological analysis for acute leukemia identification in peripheral blood microscope images. 2005 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications, CIMSA, 96-101."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
